{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- enviroment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "from scipy import spatial\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from collections import Counter\n",
    "import urllib\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/training.json\"\n",
    "dev_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/devel.json\"\n",
    "test_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/testing.json\"\n",
    "doc_file = \"/Users/alfredchen/Documents/GitHub/query-system/data/documents.json\"\n",
    "\n",
    "#save word embeddings\n",
    "embeddings = \"/Users/alfredchen/Documents/GitHub/query-system/models/mymodel-size\"\n",
    "\n",
    "# save qa log\n",
    "log_file = \"/Users/alfredchen/Documents/GitHub/query-system/data/log.txt\"\n",
    "answer_type_file = \"/Users/alfredchen/Documents/GitHub/query-system/data/answer_type_label.txt\"\n",
    "topk_file = \"/Users/alfredchen/Documents/GitHub/query-system/data/topk.json\"\n",
    "answer_file = \"/Users/alfredchen/Documents/GitHub/query-system/data/answer.txt\"\n",
    "ner_jar = \"/Users/alfredchen/Documents/GitHub/query-system/data/stanford-ner.jar\"\n",
    "ner_model = \"/Users/alfredchen/Documents/GitHub/query-system/data/english.muc.7class.distsim.crf.ser.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: preprocess\n",
    "    - sentence segmentation\n",
    "    - tokenization\n",
    "    - word level normalization\n",
    "    - creat paragraph_index\n",
    "    - create index2paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_stopwords(flag):\n",
    "        \"\"\"method for constructing stopword lookup dictionary\n",
    "        param: flag: control whether to remove \"what..\" token from stopwords\n",
    "        return: a dictionary of stop words\n",
    "        \"\"\"\n",
    "        words = list(nltk.corpus.stopwords.words())\n",
    "        if flag == False:\n",
    "        #some stopwords are helpful in targeting answer type, shall not be removed\n",
    "            words.remove('what')\n",
    "            words.remove('where')\n",
    "            words.remove('when')\n",
    "            words.remove('who')\n",
    "            words.remove('how')\n",
    "            words.remove('which')\n",
    "            # add more #\n",
    "        stopwords = {}\n",
    "        for word in words:\n",
    "            stopwords[word] = stopwords.get(word,0) + 1\n",
    "        stopwords['one'] = stopwords.get('one',0) + 1\n",
    "        stopwords['two'] = stopwords.get('one',0) + 1\n",
    "        stopwords['three'] = stopwords.get('one',0) + 1\n",
    "        stopwords['four'] = stopwords.get('one',0) + 1\n",
    "        stopwords['five'] = stopwords.get('one',0) + 1\n",
    "        stopwords['six'] = stopwords.get('one',0) + 1\n",
    "        stopwords['seven'] = stopwords.get('one',0) + 1\n",
    "        stopwords['eight'] = stopwords.get('one',0) + 1\n",
    "        stopwords['nine'] = stopwords.get('one',0) + 1\n",
    "        stopwords['ten'] = stopwords.get('one',0) + 1\n",
    "        return stopwords\n",
    "    \n",
    "stopwords = get_stopwords(False)   # do not contain what/where/when etc...\n",
    "stop_words = get_stopwords(True)   # complete stopwords\n",
    "nlp = spacy.load('en_core_web_sm') # leverage open source spacy to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    \"\"\"\n",
    "    param: word: token\n",
    "    return: lemmatized token\n",
    "    \"\"\"\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "def removePunc(string):\n",
    "    \"\"\"remove punctuation in a string\n",
    "    param: string: token, sentence\n",
    "    return: normalized string\n",
    "    \"\"\"\n",
    "    # define punctuation\n",
    "    punctuations = '''!()[]{};:\"\\/?@^&*_~'''\n",
    "    \n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct\n",
    "\n",
    "def preprocess_docs(corpus):\n",
    "    \"\"\"normalize corpus and build index to paragraph lookup\n",
    "    param: corpus: wikipedia documents\n",
    "    return: new_corpus: normalized corpus\n",
    "    return: para2index: using normalized paragraph to get it's index\n",
    "    return: index2para: using index to get normalized paragraph\n",
    "    return rpara2index: using original paragraph to get it's index\n",
    "    return: index2rpara: using index to get original paragraph\n",
    "    \"\"\"\n",
    "        para2index = {}\n",
    "        index2para = {}\n",
    "        index2rpara = {}\n",
    "        rpara2index = {}\n",
    "        new_corpus = []\n",
    "        para_len = 0\n",
    "        para_count = 0\n",
    "        for _id, doc in enumerate(corpus):\n",
    "            new_doc = []\n",
    "            for _para,para in enumerate(doc['text']):\n",
    "                rpara2index[para] = (_id,_para)         # para index\n",
    "                index2rpara[(_id,_para)] = para         # doc_id, answer_para index\n",
    "                new_para = []\n",
    "                tem_para_list = []\n",
    "                docu = nlp(para)\n",
    "                sents = [x.text for x in docu.sents]\n",
    "                #sents = para.split('.')\n",
    "                para_count += 1\n",
    "                for _sent,sent in enumerate(sents):\n",
    "                    new_sent=[]\n",
    "                    words = sent.split(' ')\n",
    "                    for word in words:\n",
    "                        new_word = lemmatize(removePunc(word.lower()))\n",
    "                        if stop_words.get(new_word) or new_word == '':\n",
    "                            continue\n",
    "                        para_len += 1\n",
    "                        new_sent.append(new_word)\n",
    "                        tem_sent = ' '.join(new_sent)\n",
    "                    new_para.append(new_sent)\n",
    "                    tem_para_list.append(tem_sent)\n",
    "                tem_para = ' '.join(tem_para_list)\n",
    "                para2index[tem_para] = (_id,_para)         # para index\n",
    "                index2para[(_id,_para)] = tem_para         # doc_id, answer_para index\n",
    "                new_doc.append(new_para)\n",
    "            new_corpus.append(new_doc)\n",
    "        average_para_len = (para_len/para_count)\n",
    "        print(\"average length of normalized paragraph:\")\n",
    "        print(average_para_len)\n",
    "        return new_corpus, para2index, index2para, rpara2index, index2rpara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.14823841059602\n"
     ]
    }
   ],
   "source": [
    "docs = json.load(open(doc_file))\n",
    "corpus,para2index,index2para,rpara2index,index2rpara = preprocess_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v(corpora,size,iter):\n",
    "    \"\"\"method for training word embeddings. \n",
    "    No need to called this method, but load a pre-trained word2vec instead\n",
    "    param: corpora: normalized wikipedia corpus\n",
    "    param: size: word embedding feature size\n",
    "    param: iter: training iteration\n",
    "    return: word2vec model\n",
    "    \"\"\"\n",
    "    docs = corpora\n",
    "    sentences = []\n",
    "    for doc in docs:\n",
    "        for sents in doc:\n",
    "            new_para = []\n",
    "            for sent in sents:\n",
    "                new_para += sent\n",
    "            sentences.append(new_para)\n",
    "    model = Word2Vec(sentences, size=size, window=5,iter=iter,workers=4)\n",
    "    model.save(embeddings+str(size)+'-iter'+str(iter))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train_w2v(corpus,160,150)   # train the model\n",
    "#load pre-trained model\n",
    "model = gensim.models.Word2Vec.load(embeddings+str(160)+'-iter'+str(150))\n",
    "model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 0.7751615643501282), ('balmoral', 0.748808741569519), ('duke', 0.7470282912254333), ('fief', 0.7448990941047668), ('royal', 0.7361407279968262), ('lambert', 0.7349075078964233), ('controversially', 0.7227569818496704), ('prince', 0.7206754088401794), ('frederick', 0.7180166244506836), ('elizabeth', 0.7173427939414978)]\n",
      "\n",
      "[('film', 0.6140520572662354), ('spielberg', 0.43539032340049744), ('console', 0.4009501338005066), ('sylvester', 0.3827870786190033), ('animate', 0.3810267746448517), ('tv', 0.37869757413864136), ('entertainment', 0.37179452180862427), ('stallone', 0.36797311902046204), ('tudio', 0.3666054904460907), ('television', 0.3636029362678528)]\n"
     ]
    }
   ],
   "source": [
    "# test our pre-trained word2vec model\n",
    "print(model.most_similar_cosmul(positive=['male', 'queen'], negative=['female']))\n",
    "print(\"\")\n",
    "print(model.most_similar(['movie']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Information Retrieval\n",
    "    - create bm25 to retrieval top N relevant paragraphs to query\n",
    "    - rank paragraphs\n",
    "    \n",
    "We created two method in relevant paragraph retrieval. One is based on distributed semantics using word2vec, another one is BM25. We tested top10 accuracy of two methods in top 1000 paragraphs in development set. Finally, we decide to take BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get TF \n",
    "def term_freqs(docid):\n",
    "    \"\"\"getting term frequency on individual documents\n",
    "    param: docid: document id\n",
    "    \"\"\"\n",
    "    paragraphs = [index2para.get((_id,_para)) for (_id,_para) in index2para.keys() if _id == docid]\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for paragraph in paragraphs:\n",
    "        #temp = nlp(paragraph)\n",
    "        tokens = paragraph.split(' ')\n",
    "        tokens = [x.strip('.').strip(',').strip('?') for x in tokens]\n",
    "        for token in tokens:\n",
    "            token = lemmatize(token.lower())\n",
    "            if not stop_words.get(token):\n",
    "                term = token\n",
    "                tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1 \n",
    "                tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1 \n",
    "        doc_id += 1\n",
    "    return tfs,doc_id+1,tfs_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build TF_IDF model\n",
    "def get_tfidf(tfs, total_docment,tfs_forward):\n",
    "    \n",
    "    para_length = {}\n",
    "    for _id,para_list in tfs_forward.items():\n",
    "        length = 0\n",
    "        for term, freq in para_list.items():\n",
    "            length += freq ** 2\n",
    "        length = length **0.5\n",
    "        para_length[_id] =  length\n",
    "    tfidf = defaultdict(dict)\n",
    "    for term, para_list in tfs.items():\n",
    "        df = len(para_list)\n",
    "        for _id, freq in para_list.items(): \n",
    "            tfidf[term][_id] = (float(tfs[term][_id]) * log(total_docment / df))# / document_length[doc_id]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_okapibm25(tf, total_docment, docid):\n",
    "    '''Calculate and return term weights based on okapibm25'''\n",
    "    k1, b, k3 = 1.5, 0.5, 0\n",
    "    okapibm25 = defaultdict(dict)\n",
    "    \n",
    "    documents = [index2para.get((_id,_para)) for (_id,_para) in index2para.keys() if _id == docid]\n",
    "    \n",
    "\n",
    "    # calculate average doc length \n",
    "    total = 0\n",
    "    for d in documents:\n",
    "        total += len(d)\n",
    "    avg_doc_length = total/len(documents)*1.0\n",
    "\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items():\n",
    "            # term occurences in query\n",
    "            # qtf = question.count(term) # SEPCIAL \n",
    "            qtf = 1.2\n",
    "            idf = log((total_docment-df+0.5) / df+0.5)\n",
    "            tf_Dt = ((k1+1)*tf[term][doc_id]) / (k1*((1-b)+b*(len(documents[doc_id])/avg_doc_length) + tf[term][doc_id]))\n",
    "            if qtf == 0:\n",
    "                third = 0\n",
    "            else:\n",
    "                third = ((k3+1)*qtf) / (k3+qtf)\n",
    "                okapibm25[term][doc_id] = idf*tf_Dt*third\n",
    "\n",
    "    return okapibm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_by_doc():\n",
    "    \"\"\"construct the bm25 term weight lookup\n",
    "    \"\"\"\n",
    "    index2tfidf = {}\n",
    "    for i in range(440):\n",
    "        tfs,total_document,tfs_forward = term_freqs(i)\n",
    "        #index2tfidf[i] = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "        index2tfidf[i] = get_okapibm25(tfs, total_document, i)\n",
    "    return index2tfidf\n",
    "        \n",
    "# construct the bm25 term weight lookup\n",
    "index2tfidf = tfidf_by_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopK(k,query,docid):\n",
    "    \"\"\"Retrieve topk paragraphs\n",
    "    param: k: how many paragraph to be retrieved\n",
    "    param: query: query\n",
    "    param: docid: document id\n",
    "    return: topk ranked relevant paragraph index\n",
    "    \"\"\"\n",
    "    top_document_id = Counter()\n",
    "    tfidf = index2tfidf.get(docid)\n",
    "    doc = nlp(query)\n",
    "    tokens = [lemmatize(x.text.lower()) for x in doc]\n",
    "    for token in tokens:\n",
    "        if not stop_words.get(token):\n",
    "            try:\n",
    "                term_tfidf = tfidf.get(token)\n",
    "                for _para, weight in term_tfidf.items():\n",
    "                     top_document_id[(docid,_para)] += weight\n",
    "            except:\n",
    "                continue\n",
    "            #for _para, weight in term_tfidf.items():\n",
    "                 #top_document_id[(docid,_para)] += weight\n",
    "    top_document=[]\n",
    "    top_document_id = top_document_id.most_common(k)\n",
    "    for content,weight in top_document_id:\n",
    "        top_document.append(content)\n",
    "    return top_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(410, 9)]\n"
     ]
    }
   ],
   "source": [
    "print(\"top1 relevant paragraph:\")\n",
    "doc = getTopK(1,'Modern browser support standards-based and defacto what?',410)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec word index\n",
    "index2word_set = set(model.index2word)\n",
    "    \n",
    "def get_doc_tfidf(docid):\n",
    "        tfidf = index2tfidf.get(docid)\n",
    "        return tfidf\n",
    "    \n",
    "def get_weight(word,docid,para_id):\n",
    "        \"\"\"get term weight in a paragraph using bm25\n",
    "        \"\"\"\n",
    "        tfidf = get_doc_tfidf(docid)\n",
    "        try:\n",
    "            weight = tfidf[word][para_id]\n",
    "        except:\n",
    "            weight = 0\n",
    "        return weight\n",
    "    \n",
    "def sent2vec(sentence, model, size, index2word_set, docid, paraid):\n",
    "        \"\"\"transform word embeddings to sentence embedding\n",
    "        param: sentence: sentence that want to be transformed\n",
    "        param: model: pre-trained word embeddings\n",
    "        param: size: feature vector dimension\n",
    "        param: index2word_set\n",
    "        return: transformed sentence vector\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            doc = nlp(sentence)\n",
    "            words = [x.text for x in doc]\n",
    "            #print(words)\n",
    "        except:\n",
    "            words = sentence\n",
    "            #pass\n",
    "       \n",
    "        feature_vec = np.zeros((size,), dtype='float32')\n",
    "        n_words = 0\n",
    "        #tfidf = get_doc_tfidf(docid)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = lemmatize(word)\n",
    "            if stop_words.get(word):\n",
    "                continue\n",
    "            if word in index2word_set:\n",
    "                n_words += 1\n",
    "                if docid != 'disable':\n",
    "                    w = get_weight(word,docid,paraid)\n",
    "                else:\n",
    "                    w = 1\n",
    "                try:\n",
    "                    feature_vec = np.add(feature_vec, model[word]*w)# weighted similarity using bm25\n",
    "                except:\n",
    "                    pass\n",
    "        if (n_words > 0):\n",
    "            feature_vec = np.divide(feature_vec, n_words)\n",
    "        return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    \"\"\"compute similarity of two string using cosine sim and embedding\n",
    "    param: v1: first string\n",
    "    param: v2: second string\n",
    "    return: similarity score\n",
    "    \"\"\"\n",
    "    num = np.dot(v1, v2)\n",
    "    d1 = np.dot(v1, v1)\n",
    "    d2 = np.dot(v2, v2)\n",
    "    if d1 > 0.0 and d2 > 0.0:\n",
    "        return num / math.sqrt(d1 * d2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (0, 1), (0, 16), (0, 23), (0, 13), (0, 11), (0, 17), (0, 2), (0, 10), (0, 14)]\n"
     ]
    }
   ],
   "source": [
    "def getTopN(N,query,docid):\n",
    "    \"\"\"method for retrieve query relevant paragraphs using distributed semantics.\n",
    "    param: N: number of paragraphs to be retrieved\n",
    "    param: query: query\n",
    "    param: docid: document id\n",
    "    return: top N ranked relevant paragraphs\n",
    "    \"\"\"\n",
    "    q = sent2vec(query, model, 160, index2word_set,'disable','disable')\n",
    "    paras = [para for (doc,para) in index2para.keys() if doc == docid]\n",
    "    sim2index = {}\n",
    "    \n",
    "    for _para in paras:\n",
    "        answer = index2para.get((docid,_para))\n",
    "        #q = sent2vec(query, model, 50, index2word_set,docid,_para)\n",
    "        answer = sent2vec(answer, model, 160, index2word_set,'disable','disable')\n",
    "        sim = sentence_similarity(q, answer)\n",
    "        sim2index[sim] = (docid,_para)\n",
    "    \n",
    "    similarity_ranks = sorted(sim2index.keys(),reverse=True)   #most similar\n",
    "    rank = []\n",
    "    for i in range(N):\n",
    "        try:\n",
    "            rank.append(sim2index.get(similarity_ranks[i]))\n",
    "        except IndexError as e:\n",
    "            print(similarity_ranks)\n",
    "    return rank\n",
    "\n",
    "index = getTopN(10,\n",
    "                \"A kilogram could be definined as having a Planck constant of what value?\",0)    \n",
    "print(index)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Top10 retrieval accuracy of BM25 and distributed semantic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_standard(dataset):\n",
    "    file = json.load(open(dataset))\n",
    "    gold2index={}\n",
    "    for line in file:\n",
    "        _id = line['docid']\n",
    "        try:\n",
    "            _para = line['answer_paragraph']\n",
    "        except KeyError:\n",
    "            _para = line['id']\n",
    "        gold2index[line['question']]=(_id,_para)\n",
    "    return gold2index\n",
    "\n",
    "def my_qa(n,dataset,flag):\n",
    "    file = json.load(open(dataset))\n",
    "    qa2index={}\n",
    "    for i in range(n):\n",
    "        save = open(log_file,\"a\")\n",
    "        line = file[i]\n",
    "        query = line['question']\n",
    "        if flag == 1:\n",
    "            possible = getTopK(1,query,line['docid'])\n",
    "        elif flag == 2:\n",
    "            possible = getTopN(1,query, line['docid'])\n",
    "        qa2index[line['question']] = possible\n",
    "        record = line['question']+':'+str(possible)+'\\n'\n",
    "        save.write(record)\n",
    "        save.close()\n",
    "    return qa2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(gold_standard,qas):\n",
    "    query = qas.keys()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i,q in enumerate(query):\n",
    "        gold = gold_standard.get(q)\n",
    "        if gold in qas.get(q):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    acc = correct/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurcacy of semantic:\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "#test on top 100 devset\n",
    "print(\"accurcacy of semantic:\")\n",
    "gold_index = gold_standard(dev_set)\n",
    "qa_index = my_qa(100,dev_set,2)\n",
    "accu = acc(gold_index,qa_index)\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurcacy of BM25:\n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "#test on top 100 devset\n",
    "print(\"accurcacy of BM25:\")\n",
    "gold_index = gold_standard(dev_set)\n",
    "qa_index = my_qa(100,dev_set,1)\n",
    "accu = acc(gold_index,qa_index)\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Named Entity Recognition and Answer Type Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part, we focused on extract potential answer terms from retrieved relevant paragraphs. We leverage open source spacy in helping us doing named entity recognition, noun-phrase chunk extraction. We use machine learning to classify answer type of query using top 18174 training data. The feature of this task is distributed semantics (word embedding). The answer types are shown below:\n",
    "\n",
    "\n",
    "    - PERSON              -People, including fictional.\n",
    "    - NORP\t              -Nationalities or religious or political groups.\n",
    "    - FACILITY\t          -Buildings, airports, highways, bridges, etc.\n",
    "    - ORG\t              -Companies, agencies, institutions, etc.\n",
    "    - GPE\t              -Countries, cities, states.\n",
    "    - LOC\t              -Non-GPE locations, mountain ranges, bodies of water.\n",
    "    - PRODUCT\t          -Objects, vehicles, foods, etc. (Not services.)\n",
    "    - EVENT\t              -Named hurricanes, battles, wars, sports events, etc.\n",
    "    - WORK_OF_ART\t      -Titles of books, songs, etc.\n",
    "    - LAW\tNamed         -documents made into laws.\n",
    "    - LANGUAGE\t          -Any named language.\n",
    "    - DATE\t              -Absolute or relative dates or periods.\n",
    "    - TIME\t              -Times smaller than a day.\n",
    "    - PERCENT\t          -Percentage, including \"%\".\n",
    "    - MONEY\t              -Monetary values, including unit.\n",
    "    - QUANTITY\t          -Measurements, as of weight or distance.\n",
    "    - ORDINAL\t          -\"first\", \"second\", etc.\n",
    "    - CARDINAL\t          -Numerals that do not fall under another type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up classifier for detect answer type\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#preprocess word in query\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"method query processing\n",
    "    param: sentence: original query\n",
    "    return: processed query\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)     # tokenization\n",
    "    words = [x.text for x in doc]\n",
    "    words = [lemmatize(removePunc(x.lower())) for x in words]   #normalization\n",
    "    words = [x for x in words if not stopwords.get(x) and x != '']\n",
    "    return ' '.join(words)\n",
    "\n",
    "def named_entity_recognize(sentence,valid_ner):\n",
    "    \"\"\"method for extract named entity from relevant paragraphs\n",
    "    param: sentence: texture paragraphs or sentence\n",
    "    param: valid_ner: type of named entity to extract\n",
    "    return: a list of named entities\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    ner_tag = []\n",
    "    sentence = removePunc(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    # case for tagging training data for answer type classification task\n",
    "    if valid_ner == 'disable':\n",
    "        ner_tag = ''\n",
    "        for ent in doc.ents:\n",
    "            ner_tag=ent.label_\n",
    "        if ner_tag == '':\n",
    "            ner_tag = 'O'\n",
    "        return ner_tag\n",
    "    \n",
    "    # case not 'O'\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in valid_ner:\n",
    "            entities.append(lemmatize(removePunc(ent.text.lower())))\n",
    "            ner_tag.append(ent.label_)\n",
    "            \n",
    "    # case 'O', return noun_phrase chunk(start with adj/propn and end with noun)\n",
    "    if valid_ner == 'O':\n",
    "        for chunk in doc.noun_chunks:\n",
    "            #print(chunk.text)\n",
    "            proc = nlp(chunk.text)\n",
    "            flag = 1\n",
    "            for ent in proc.ents:\n",
    "                if ent:\n",
    "                    flag = 0\n",
    "\n",
    "            if flag == 1:\n",
    "                entities.append(chunk.text.lower())\n",
    "    if entities == []:\n",
    "        return 'unknown'\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john smith', 'mike johnson', 'january, 11th 2017']\n",
      "['essonne', '814 square kilometers']\n"
     ]
    }
   ],
   "source": [
    "# test on the named entity recognize\n",
    "entities = named_entity_recognize(\"John Smith, Mike Johnson, January, 11th 2017\",['DATE'])  \n",
    "print(entities)\n",
    "\n",
    "context = index2rpara.get((14,23))\n",
    "#print(context)\n",
    "\n",
    "entities = named_entity_recognize(context,['QUANTITY','PERSON'])\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use ner to tag answers of training data and assume the result is gold standard. We use these tags as labels to perform a supervised machine learning query answer type classification task. In order to justify the mistakes of the ner tags, we also apply a rule to amend labels. For instance, queries containing when is obviously an answer type of 'DATE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ner recognition to get all answer type from training data\n",
    "\"\"\"\n",
    "def tag_answer(dataset):\n",
    "    file = json.load(open(dataset))\n",
    "    for _id,line in enumerate(file):\n",
    "        answer = line['text']\n",
    "        label = named_entity_recognize(answer,'disable')\n",
    "        f = open(answer_type,\"a\")\n",
    "        f.write(str(_id)+':'+str(label)+'\\n')\n",
    "        f.close()\n",
    "    print('done')\n",
    "    return True\n",
    "\n",
    "tag_answer(train_set)\n",
    "\"\"\" \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "            'name':'PERSON',\n",
    "            'country': 'GPE',\n",
    "            'capital': 'GPE',\n",
    "            'newspaper':'ORG',\n",
    "            'company':'ORG',\n",
    "            'city': 'GPE',\n",
    "            'person':'PERSON',\n",
    "            'location': ['LOC','GPE'],\n",
    "            'mountain':'LOC',\n",
    "            'website':'ORG',\n",
    "            'airline':'ORG',\n",
    "            'which organization': 'ORG',\n",
    "            'where': 'GPE',\n",
    "            'when': 'DATE',           \n",
    "            'who': 'PERSON',     \n",
    "            'what scientist':'PERSON',\n",
    "            'what time':'DATE',\n",
    "            'year':'DATE',\n",
    "            'what athlete':'PERSON',\n",
    "            'which athlete':'PERSON',\n",
    "            'percentage':'PERCENT',\n",
    "            'what people': 'PERSON',\n",
    "            'what date':'DATE',\n",
    "            'what day':'DATE',\n",
    "            'what year': 'DATE',\n",
    "            'what city' : 'GPE',\n",
    "            'which company': 'ORG' ,\n",
    "            'which publication':'ORG',\n",
    "            'what government':['ORG','NORP'],\n",
    "            'which supporters' : 'PERSON',\n",
    "            'which footballer': 'PERSON',\n",
    "            'which actor':'PERSON',\n",
    "            'Which actress':'PERSON',\n",
    "            'which American actress':'PERSON',\n",
    "            'what activists':'PERSON',\n",
    "            'which team member' : 'PERSON',\n",
    "            'what football star': 'PERSON',\n",
    "            'which blogger': 'PERSON',\n",
    "            'which torchbearer':'PERSON',\n",
    "            'which wheelchair-bound torchbearer' : 'PERSON',\n",
    "            'how much of': ['CARDINAL','MONEY','QUANTITY','PERCENT'],\n",
    "            'by how much': ['CARDINAL','MONEY','QUANTITY','PERCENT'],\n",
    "            'how many':['CARDINAL','QUANTITY','PERCENT'],\n",
    "            'value':['CARDINAL','MONEY','PERCENT'],\n",
    "            'rank':['CARDINAL'],\n",
    "            'how much': 'O'\n",
    "            \n",
    "            \n",
    "}\n",
    "\n",
    "money_list = ['cost', 'worth', 'spend', 'money', 'worth', 'invest']\n",
    "\n",
    "def tag_answer_type(question):\n",
    "    answer_type = 'O'\n",
    "    processed_question = []\n",
    "    processed_question_str = None\n",
    "    for token in [question]:\n",
    "        processed_question.append(token.lower())\n",
    "    processed_question_str = \" \".join(x for x in processed_question)\n",
    "    for k,v in rules.items():\n",
    "        if k in processed_question_str:\n",
    "            #print(k)\n",
    "            if k == 'how much':\n",
    "                for item in money_list:\n",
    "                    #print(\"item\", item)\n",
    "                    if item in processed_question_str:\n",
    "                        answer_type = 'MONEY'\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                answer_type = rules.get(k, \"O\")    \n",
    "    return answer_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells show the process of training answer type classifier. Do not need to run these cells, just load a pre-trained classifier in later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18174\n",
      "18174\n"
     ]
    }
   ],
   "source": [
    "# train a classifier to recognize answer type from query using top 18173 data)\n",
    "def query2vector(dataset):\n",
    "    \"\"\"method for transforming texture query into embeddings\n",
    "    param: dataset: training.json\n",
    "    return: X: queries feature matrix\n",
    "    return: Y: labels\n",
    "    \"\"\"\n",
    "    label =  open(answer_type_file)\n",
    "    train = json.load(open(dataset))\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i,line in enumerate(label):\n",
    "        #query = preprocess_sentence(train[i]['question'])\n",
    "        x = sent2vec(train[i]['question'], model, 160, index2word_set, 'disable','disable')\n",
    "        y = line.split(':')[1].strip()\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    label.close()\n",
    "    return X,Y\n",
    "    \n",
    "vectors,labels = query2vector(train_set)\n",
    "print(len(vectors))\n",
    "print(len(labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7793000990425883\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# supervised logistic regression classifier\n",
    "def train(X,y):\n",
    "    \"\"\"using grid search to find best hyperparameter\n",
    "    \"\"\"\n",
    "    # Create regularization penalty space\n",
    "    penalty = ['l1', 'l2']\n",
    "    # Create regularization hyperparameter space\n",
    "    C = np.logspace(0, 4, 10)\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict(C=C, penalty=penalty)\n",
    "    # Create grid search using 5-fold cross validation\n",
    "    clf = LogisticRegression()\n",
    "    grid = GridSearchCV(clf, hyperparameters, cv=5, verbose=0)\n",
    "    # Fit grid search\n",
    "    best_model = grid.fit(X, y)\n",
    "    # View best hyperparameters\n",
    "    print(grid.best_score_)\n",
    "    return best_model\n",
    "            \n",
    "answer_type_clf = train(vectors,labels)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start running from this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save clf \n",
    "import joblib\n",
    "#joblib.dump(answer_type_clf, '/Users/alfredchen/Documents/GitHub/query-system/models/relevance.pkl',protocol=2)\n",
    "#load clf\n",
    "answer_type_clf= joblib.load('/Users/alfredchen/Documents/GitHub/query-system/models/relevance.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "def pred_answer_type(query,classifier):\n",
    "    \"\"\"semantic machine learning and rule-based answer type classifier\n",
    "    \"\"\"\n",
    "    q = sent2vec(query, model, 160, index2word_set, 'disable','disable')\n",
    "    preds = classifier.predict([q][0:])\n",
    "    if preds[0] == 'O':\n",
    "        pred=tag_answer_type(query)\n",
    "    else:\n",
    "        pred = preds[0]\n",
    "    return pred\n",
    "\n",
    "print(pred_answer_type('who first observed the photoelectric effect?',answer_type_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI: Answer Ranking Pipeline\n",
    "This part, we create a pipeline to process texture query and a list of potential answers to output a single most probable answer. Three filter is contained in the pipeline. Firstly, we will remove answers overlap with query. Secondly, we will remove answers semantically unrelated to query. Thirdly, we set up a co-occurrence method to rank rest answers. If an answer co-occur with many query terms in a sentence boundary, it is more likely to be the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = get_stopwords(False)\n",
    "def first_filter(query,answer):\n",
    "    \"\"\"filter out answers overlap with query\n",
    "    param: query\n",
    "    param: answer: a list of answers\n",
    "    return: a list of filterd answers\n",
    "    \"\"\"\n",
    "    query = preprocess_sentence(query).split(' ')\n",
    "    q = []\n",
    "    a = []\n",
    "    template = []\n",
    "    for w in query:\n",
    "        if not stop_words.get(w):\n",
    "            lemmatize(removePunc(w.lower()))\n",
    "            q.append(w)\n",
    "\n",
    "    for w in answer:\n",
    "        try:\n",
    "            flag = 1\n",
    "            words = w.split(' ')\n",
    "            #words = [lemmatize(removePunc(x).lower()) for x in words]\n",
    "            for word in words:\n",
    "                if not stop_words.get(word):\n",
    "                    if word not in template:\n",
    "                        if word in q:\n",
    "                            flag = 0\n",
    "                    else:\n",
    "                        flag = 0\n",
    "                else:\n",
    "                    flag = 0\n",
    "            if flag == 1:\n",
    "                a.append(w)\n",
    "                template.append(word)\n",
    "        except:\n",
    "            print(\"exception in first filter\")                \n",
    "    q = ' '.join(q)\n",
    "    return q,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def second_filter(query,answer):\n",
    "    \"\"\"\n",
    "    param: query\n",
    "    param: answer: a list of answers\n",
    "    return: a list of filterd answers\n",
    "    \"\"\"\n",
    "    \n",
    "    query, answer = first_filter(query,answer)\n",
    "    query_semantic = sent2vec(query, model, 160, index2word_set, 'disable','disable')\n",
    "    max_sim = 0\n",
    "    potential_answer = []\n",
    "    sim_answer = ''\n",
    "    try:        \n",
    "        for a in answer:\n",
    "            ans = sent2vec(a, model, 160, index2word_set, 'disable','disable')\n",
    "            sim = cosine_similarity(query_semantic,ans)\n",
    "            if sim >= max_sim:\n",
    "                potential_answer.append(a)\n",
    "    except:\n",
    "        print(\"exception in second filter\")\n",
    "    return query, potential_answer\n",
    "\n",
    "\n",
    "def co_occurrence(query,answer,context,docid,paraid):\n",
    "    \"\"\"\n",
    "    answer ranking using co_occurrence\n",
    "    \"\"\"\n",
    "    key_words = query.split(' ')\n",
    "    #tfidf = get_doc_tfidf(docid)\n",
    "    doc = nlp(context)\n",
    "    sentences = [x.text for x in doc.sents]\n",
    "    co_occurrence_score = {}\n",
    "    for ans in answer:\n",
    "        score = 200\n",
    "        try:\n",
    "            weight = get_weight(ans.split(' ')[-1],docid,paraid)  # noun-phrase, weight on the last token\n",
    "        except:\n",
    "            weight = get_weight(ans,docid,paraid)\n",
    "        for sent in sentences:\n",
    "            for word in key_words:\n",
    "                w2 = get_weight(word,docid,paraid)\n",
    "                try:\n",
    "                    if word in sent and ans in sent:\n",
    "                        score -= weight\n",
    "                except:\n",
    "                    continue\n",
    "        co_occurrence_score[score] = ans\n",
    "    return co_occurrence_score\n",
    "        \n",
    "    \n",
    "def formalize_answers(answer_list):\n",
    "    \"\"\"take highest ranking answer\n",
    "    \"\"\"    \n",
    "    similarity_ranks = sorted(answer_list.keys(),reverse=False)   #sims\n",
    "    rank = []\n",
    "    for i in range(1):\n",
    "        try:\n",
    "            rank.append(answer_list.get(similarity_ranks[i]))\n",
    "        except IndexError as e:\n",
    "            result = 'unknown'\n",
    "    try:\n",
    "        result = rank[0]\n",
    "    except:\n",
    "        result = 'unknown'\n",
    "    \n",
    "    result = lemmatize(removePunc(result.lower()))\n",
    "    final = ''\n",
    "    if result[0] is '%':\n",
    "        final += '% '\n",
    "    else:\n",
    "        final += result[0]\n",
    "    for i in range(1,len(result)):\n",
    "        if result[i] is '%':\n",
    "            final += ' %'\n",
    "        else:\n",
    "            final += result[i]\n",
    "    return final   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Combine Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7)]\n",
      "heinrich hertz\n"
     ]
    }
   ],
   "source": [
    "def extract_answer_from_topk(query,topk):\n",
    "    \"\"\"method to combine entire retrieval, ner, pipeline\n",
    "    param: query\n",
    "    param: topk: a list of paragraph index\n",
    "    return: one answer\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    # paragraph retrieval\n",
    "    for index in topk:\n",
    "        context += index2rpara.get(tuple(index))\n",
    "    # machine learning and rule-based hyibrid NER\n",
    "    valid_ner = pred_answer_type(query,answer_type_clf)\n",
    "    #if valid_ner == 'O':\n",
    "        #valid_ner = tag_answer_type(query)\n",
    "    # answer extraction\n",
    "    potential_answers = named_entity_recognize(context,valid_ner)\n",
    "    # answer ranking\n",
    "    query, answer = second_filter(query,potential_answers)\n",
    "    ans = {}\n",
    "    score = {}\n",
    "    context = \"\"\n",
    "    for index in topk:\n",
    "        context = index2para.get(index)\n",
    "        ans.update(co_occurrence(query, answer, context,index[0],index[1]))\n",
    "    #answer finalization\n",
    "    final = formalize_answers(ans)\n",
    "    return final\n",
    "\n",
    "topk = getTopK(1,\"who first observed the photoelectric effect?\",0)\n",
    "print(topk)\n",
    "query = \"who first observed the photoelectric effect?\"\n",
    "\n",
    "answers = extract_answer_from_topk(query,topk)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n"
     ]
    }
   ],
   "source": [
    "# test acc on devset\n",
    "import csv\n",
    "\n",
    "def test_answers(dataset):\n",
    "    \"\"\"question answering system\n",
    "    param: dataset\n",
    "    return: write into .csv file\n",
    "    \"\"\"\n",
    "    file = json.load(open(dataset))\n",
    "    gold = []\n",
    "    pred = []\n",
    "    csv_file = open('output.csv', 'a')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['id', 'answer'])\n",
    "    csv_file.close()\n",
    "\n",
    "    for _id,line in enumerate(file):\n",
    "        query = line['question']\n",
    "        topk = getTopK(1,query,line['docid'])\n",
    "        #print(topk)\n",
    "        answer = extract_answer_from_topk(query,topk)\n",
    "        #gold.append(line['text'])\n",
    "        pred.append(answer)\n",
    "        #print(query)\n",
    "        \n",
    "        csv_file = open('output.csv', 'a')\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([_id, answer])\n",
    "        #print(_id)\n",
    "        if _id % 50 == 0:\n",
    "            print(_id)\n",
    "            #print(metrics.accuracy_score(gold,pred))\n",
    "        csv_file.close()\n",
    "    return gold,pred\n",
    "\n",
    "gold,pred = test_answers(test_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrices(y_test,y_pred): \n",
    "    print(\"Accuracy:\") \n",
    "    print(metrics.accuracy_score(y_test,y_pred))\n",
    "    \n",
    "    print(\"\\nAverage precision:\")\n",
    "    print(metrics.precision_score(y_test,y_pred,average='macro'))\n",
    "\n",
    "    print(\"\\nAverage recall:\")\n",
    "    print(metrics.recall_score(y_test,y_pred,average='macro'))\n",
    "\n",
    "    print(\"\\nAverage f1:\")\n",
    "    print(metrics.f1_score(y_test,y_pred,average='macro'))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
